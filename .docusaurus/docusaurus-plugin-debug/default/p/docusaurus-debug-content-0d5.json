{"allContent":{"docusaurus-plugin-css-cascade-layers":{},"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/docs","tagsPath":"/docs/tags","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"C:\\Hackathon\\Humanoid-Robotics-Book\\Humanoid-Robotics-Book\\sidebars.js","contentPath":"C:\\Hackathon\\Humanoid-Robotics-Book\\Humanoid-Robotics-Book\\docs","docs":[{"id":"capstone/guide","title":"Capstone: Humanoid Robot Simulation","description":"This capstone project will guide you through building a complete simulation of a humanoid robot that integrates all the concepts learned in the previous modules. You will create a robot that can hear a voice command, understand it, plan a path, navigate obstacles, identify an object, and interact with the environment.","source":"@site/docs/capstone/guide.mdx","sourceDirName":"capstone","slug":"/capstone/guide","permalink":"/docs/capstone/guide","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/capstone/guide.mdx","tags":[],"version":"current","frontMatter":{"title":"Capstone: Humanoid Robot Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Multi-Step Reasoning for Robotics","permalink":"/docs/module-4-vla/reasoning"}},{"id":"index","title":"Physical AI & Humanoid Robotics","description":"Welcome to the Physical AI & Humanoid Robotics textbook. This comprehensive guide covers the essential concepts and practical implementations for building, simulating, and deploying AI systems in physical humanoid robots using ROS 2, Gazebo, Unity, and NVIDIA Isaac.","source":"@site/docs/index.mdx","sourceDirName":".","slug":"/","permalink":"/docs/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/index.mdx","tags":[],"version":"current","frontMatter":{"title":"Physical AI & Humanoid Robotics","slug":"/"},"sidebar":"tutorialSidebar","next":{"title":"Module 1: The Robotic Nervous System - ROS 2","permalink":"/docs/module-1-ros2/introduction"}},{"id":"module-1-ros2/architecture","title":"ROS 2 Architecture","description":"ROS 2 employs a distributed architecture where different software components communicate with each other through a middleware layer. The core concepts of ROS 2 architecture include:","source":"@site/docs/module-1-ros2/architecture.mdx","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/architecture","permalink":"/docs/module-1-ros2/architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-1-ros2/architecture.mdx","tags":[],"version":"current","frontMatter":{"title":"ROS 2 Architecture"},"sidebar":"tutorialSidebar","previous":{"title":"Module 1: The Robotic Nervous System - ROS 2","permalink":"/docs/module-1-ros2/introduction"},"next":{"title":"rclpy Code Examples","permalink":"/docs/module-1-ros2/code_examples"}},{"id":"module-1-ros2/code_examples","title":"rclpy Code Examples","description":"This page provides code examples for using rclpy, the Python client library for ROS 2.","source":"@site/docs/module-1-ros2/code_examples.mdx","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/code_examples","permalink":"/docs/module-1-ros2/code_examples","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-1-ros2/code_examples.mdx","tags":[],"version":"current","frontMatter":{"title":"rclpy Code Examples"},"sidebar":"tutorialSidebar","previous":{"title":"ROS 2 Architecture","permalink":"/docs/module-1-ros2/architecture"},"next":{"title":"URDF for Humanoid Robots","permalink":"/docs/module-1-ros2/urdf"}},{"id":"module-1-ros2/introduction","title":"Module 1: The Robotic Nervous System - ROS 2","description":"Welcome to the first module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore the foundations of robotic middleware using ROS 2 (Robot Operating System 2).","source":"@site/docs/module-1-ros2/introduction.mdx","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/introduction","permalink":"/docs/module-1-ros2/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-1-ros2/introduction.mdx","tags":[],"version":"current","frontMatter":{"title":"Module 1: The Robotic Nervous System - ROS 2"},"sidebar":"tutorialSidebar","previous":{"title":"Physical AI & Humanoid Robotics","permalink":"/docs/"},"next":{"title":"ROS 2 Architecture","permalink":"/docs/module-1-ros2/architecture"}},{"id":"module-1-ros2/packages","title":"Creating and Running ROS 2 Packages","description":"In ROS 2, code is organized into packages. A package is a directory that contains a package.xml file, which describes the package, and a CMakeLists.txt file, which specifies how to build the package.","source":"@site/docs/module-1-ros2/packages.mdx","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/packages","permalink":"/docs/module-1-ros2/packages","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-1-ros2/packages.mdx","tags":[],"version":"current","frontMatter":{"title":"Creating and Running ROS 2 Packages"},"sidebar":"tutorialSidebar","previous":{"title":"URDF for Humanoid Robots","permalink":"/docs/module-1-ros2/urdf"},"next":{"title":"Module 2: The Digital Twin - Gazebo and Unity","permalink":"/docs/module-2-gazebo/introduction"}},{"id":"module-1-ros2/urdf","title":"URDF for Humanoid Robots","description":"URDF (Unified Robot Description Format) is an XML file format used in ROS to describe the physical structure of a robot. A URDF file specifies the links, joints, and visual and collision properties of a robot.","source":"@site/docs/module-1-ros2/urdf.mdx","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/urdf","permalink":"/docs/module-1-ros2/urdf","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-1-ros2/urdf.mdx","tags":[],"version":"current","frontMatter":{"title":"URDF for Humanoid Robots"},"sidebar":"tutorialSidebar","previous":{"title":"rclpy Code Examples","permalink":"/docs/module-1-ros2/code_examples"},"next":{"title":"Creating and Running ROS 2 Packages","permalink":"/docs/module-1-ros2/packages"}},{"id":"module-2-gazebo/introduction","title":"Module 2: The Digital Twin - Gazebo and Unity","description":"Welcome to the second module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore the world of robot simulation using Gazebo and Unity.","source":"@site/docs/module-2-gazebo/introduction.mdx","sourceDirName":"module-2-gazebo","slug":"/module-2-gazebo/introduction","permalink":"/docs/module-2-gazebo/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-gazebo/introduction.mdx","tags":[],"version":"current","frontMatter":{"title":"Module 2: The Digital Twin - Gazebo and Unity"},"sidebar":"tutorialSidebar","previous":{"title":"Creating and Running ROS 2 Packages","permalink":"/docs/module-1-ros2/packages"},"next":{"title":"Setting up Gazebo and Building Environments","permalink":"/docs/module-2-gazebo/setup"}},{"id":"module-2-gazebo/physics","title":"Physics Concepts in Robotics Simulation","description":"In robotics simulation, understanding fundamental physics concepts is crucial for creating realistic and accurate simulations. Here, we will explore gravity, collision, and friction.","source":"@site/docs/module-2-gazebo/physics.mdx","sourceDirName":"module-2-gazebo","slug":"/module-2-gazebo/physics","permalink":"/docs/module-2-gazebo/physics","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-gazebo/physics.mdx","tags":[],"version":"current","frontMatter":{"title":"Physics Concepts in Robotics Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Setting up Gazebo and Building Environments","permalink":"/docs/module-2-gazebo/setup"},"next":{"title":"Simulating Sensors in Gazebo","permalink":"/docs/module-2-gazebo/sensors"}},{"id":"module-2-gazebo/sensors","title":"Simulating Sensors in Gazebo","description":"Robots rely on sensors to perceive their environment. In simulation, we can model different types of sensors to mimic real-world sensor data. Here, we will explore simulating LiDAR, IMU, and depth cameras in Gazebo.","source":"@site/docs/module-2-gazebo/sensors.mdx","sourceDirName":"module-2-gazebo","slug":"/module-2-gazebo/sensors","permalink":"/docs/module-2-gazebo/sensors","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-gazebo/sensors.mdx","tags":[],"version":"current","frontMatter":{"title":"Simulating Sensors in Gazebo"},"sidebar":"tutorialSidebar","previous":{"title":"Physics Concepts in Robotics Simulation","permalink":"/docs/module-2-gazebo/physics"},"next":{"title":"High-Fidelity Simulation with Unity","permalink":"/docs/module-2-gazebo/unity"}},{"id":"module-2-gazebo/setup","title":"Setting up Gazebo and Building Environments","description":"Gazebo is a popular open-source robot simulator that allows you to create realistic 3D environments and simulate the behavior of robots in those environments.","source":"@site/docs/module-2-gazebo/setup.mdx","sourceDirName":"module-2-gazebo","slug":"/module-2-gazebo/setup","permalink":"/docs/module-2-gazebo/setup","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-gazebo/setup.mdx","tags":[],"version":"current","frontMatter":{"title":"Setting up Gazebo and Building Environments"},"sidebar":"tutorialSidebar","previous":{"title":"Module 2: The Digital Twin - Gazebo and Unity","permalink":"/docs/module-2-gazebo/introduction"},"next":{"title":"Physics Concepts in Robotics Simulation","permalink":"/docs/module-2-gazebo/physics"}},{"id":"module-2-gazebo/unity","title":"High-Fidelity Simulation with Unity","description":"Unity is a powerful game engine that can also be used for high-fidelity robotics simulation. It provides a rich set of tools and features for creating realistic and interactive environments.","source":"@site/docs/module-2-gazebo/unity.mdx","sourceDirName":"module-2-gazebo","slug":"/module-2-gazebo/unity","permalink":"/docs/module-2-gazebo/unity","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-gazebo/unity.mdx","tags":[],"version":"current","frontMatter":{"title":"High-Fidelity Simulation with Unity"},"sidebar":"tutorialSidebar","previous":{"title":"Simulating Sensors in Gazebo","permalink":"/docs/module-2-gazebo/sensors"},"next":{"title":"Module 3: The AI-Robot Brain - NVIDIA Isaac","permalink":"/docs/module-3-isaac/introduction"}},{"id":"module-3-isaac/introduction","title":"Module 3: The AI-Robot Brain - NVIDIA Isaac","description":"Welcome to the third module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore the use of NVIDIA Isaac for AI perception and navigation in robotics.","source":"@site/docs/module-3-isaac/introduction.mdx","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/introduction","permalink":"/docs/module-3-isaac/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/introduction.mdx","tags":[],"version":"current","frontMatter":{"title":"Module 3: The AI-Robot Brain - NVIDIA Isaac"},"sidebar":"tutorialSidebar","previous":{"title":"High-Fidelity Simulation with Unity","permalink":"/docs/module-2-gazebo/unity"},"next":{"title":"Using NVIDIA Isaac Sim","permalink":"/docs/module-3-isaac/isaac_sim"}},{"id":"module-3-isaac/isaac_sim","title":"Using NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a powerful, photorealistic robot simulator that is part of the NVIDIA Isaac robotics platform. It provides a virtual environment for developing, testing, and training AI-powered robots.","source":"@site/docs/module-3-isaac/isaac_sim.mdx","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/isaac_sim","permalink":"/docs/module-3-isaac/isaac_sim","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/isaac_sim.mdx","tags":[],"version":"current","frontMatter":{"title":"Using NVIDIA Isaac Sim"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: The AI-Robot Brain - NVIDIA Isaac","permalink":"/docs/module-3-isaac/introduction"},"next":{"title":"Generating Synthetic Data with NVIDIA Isaac Sim","permalink":"/docs/module-3-isaac/synthetic_data"}},{"id":"module-3-isaac/rl","title":"Reinforcement Learning for Robotics with NVIDIA Isaac","description":"Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions in an environment to maximize a reward signal. RL is well-suited for robotics tasks, as it allows robots to learn complex behaviors through trial and error.","source":"@site/docs/module-3-isaac/rl.mdx","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/rl","permalink":"/docs/module-3-isaac/rl","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/rl.mdx","tags":[],"version":"current","frontMatter":{"title":"Reinforcement Learning for Robotics with NVIDIA Isaac"},"sidebar":"tutorialSidebar","previous":{"title":"VSLAM and Navigation with NVIDIA Isaac","permalink":"/docs/module-3-isaac/vslam"},"next":{"title":"The Sim-to-Real Pipeline","permalink":"/docs/module-3-isaac/sim2real"}},{"id":"module-3-isaac/sim2real","title":"The Sim-to-Real Pipeline","description":"The sim-to-real pipeline refers to the process of transferring AI models trained in simulation to real-world robots. This is a crucial step in robotics development, as it allows you to leverage the benefits of simulation while still deploying your models on real robots.","source":"@site/docs/module-3-isaac/sim2real.mdx","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/sim2real","permalink":"/docs/module-3-isaac/sim2real","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/sim2real.mdx","tags":[],"version":"current","frontMatter":{"title":"The Sim-to-Real Pipeline"},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning for Robotics with NVIDIA Isaac","permalink":"/docs/module-3-isaac/rl"},"next":{"title":"Module 4: Vision-Language-Action","permalink":"/docs/module-4-vla/introduction"}},{"id":"module-3-isaac/synthetic_data","title":"Generating Synthetic Data with NVIDIA Isaac Sim","description":"Synthetic data is artificially created data that can be used to train AI models. Generating synthetic data is a powerful technique for robotics, as it allows you to create large datasets of labeled data without the need for real-world data collection.","source":"@site/docs/module-3-isaac/synthetic_data.mdx","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/synthetic_data","permalink":"/docs/module-3-isaac/synthetic_data","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/synthetic_data.mdx","tags":[],"version":"current","frontMatter":{"title":"Generating Synthetic Data with NVIDIA Isaac Sim"},"sidebar":"tutorialSidebar","previous":{"title":"Using NVIDIA Isaac Sim","permalink":"/docs/module-3-isaac/isaac_sim"},"next":{"title":"VSLAM and Navigation with NVIDIA Isaac","permalink":"/docs/module-3-isaac/vslam"}},{"id":"module-3-isaac/vslam","title":"VSLAM and Navigation with NVIDIA Isaac","description":"VSLAM (Visual Simultaneous Localization and Mapping) is a technique used by robots to build a map of their environment while simultaneously estimating their pose within that map. Navigation builds upon the generated map, allowing the robot to plan and execute paths to reach desired goals. NVIDIA Isaac provides tools for implementing VSLAM and navigation algorithms.","source":"@site/docs/module-3-isaac/vslam.mdx","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/vslam","permalink":"/docs/module-3-isaac/vslam","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-isaac/vslam.mdx","tags":[],"version":"current","frontMatter":{"title":"VSLAM and Navigation with NVIDIA Isaac"},"sidebar":"tutorialSidebar","previous":{"title":"Generating Synthetic Data with NVIDIA Isaac Sim","permalink":"/docs/module-3-isaac/synthetic_data"},"next":{"title":"Reinforcement Learning for Robotics with NVIDIA Isaac","permalink":"/docs/module-3-isaac/rl"}},{"id":"module-4-vla/introduction","title":"Module 4: Vision-Language-Action","description":"Welcome to the fourth module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore how to combine vision, language, and action to create robots that can understand and interact with the world in a more natural way.","source":"@site/docs/module-4-vla/introduction.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/introduction","permalink":"/docs/module-4-vla/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/introduction.mdx","tags":[],"version":"current","frontMatter":{"title":"Module 4: Vision-Language-Action"},"sidebar":"tutorialSidebar","previous":{"title":"The Sim-to-Real Pipeline","permalink":"/docs/module-3-isaac/sim2real"},"next":{"title":"Voice Recognition with Whisper","permalink":"/docs/module-4-vla/whisper"}},{"id":"module-4-vla/llm_planning","title":"LLM Planning for Robotics","description":"Large Language Models (LLMs) can be used to generate robot plans from natural language commands. This allows robots to perform complex tasks without the need for explicit programming.","source":"@site/docs/module-4-vla/llm_planning.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/llm_planning","permalink":"/docs/module-4-vla/llm_planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/llm_planning.mdx","tags":[],"version":"current","frontMatter":{"title":"LLM Planning for Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Voice Recognition with Whisper","permalink":"/docs/module-4-vla/whisper"},"next":{"title":"Multi-Step Reasoning for Robotics","permalink":"/docs/module-4-vla/reasoning"}},{"id":"module-4-vla/reasoning","title":"Multi-Step Reasoning for Robotics","description":"Multi-step reasoning is a critical capability for robots that need to perform complex tasks. It involves breaking down a high-level goal into a sequence of smaller, more manageable sub-tasks, and then executing those sub-tasks in a logical order.","source":"@site/docs/module-4-vla/reasoning.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/reasoning","permalink":"/docs/module-4-vla/reasoning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/reasoning.mdx","tags":[],"version":"current","frontMatter":{"title":"Multi-Step Reasoning for Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"LLM Planning for Robotics","permalink":"/docs/module-4-vla/llm_planning"},"next":{"title":"Capstone: Humanoid Robot Simulation","permalink":"/docs/capstone/guide"}},{"id":"module-4-vla/whisper","title":"Voice Recognition with Whisper","description":"Whisper is a neural network developed by OpenAI that transcribes spoken language into text. It is a powerful tool for enabling robots to understand and respond to human voice commands.","source":"@site/docs/module-4-vla/whisper.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/whisper","permalink":"/docs/module-4-vla/whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/whisper.mdx","tags":[],"version":"current","frontMatter":{"title":"Voice Recognition with Whisper"},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action","permalink":"/docs/module-4-vla/introduction"},"next":{"title":"LLM Planning for Robotics","permalink":"/docs/module-4-vla/llm_planning"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"index"},{"type":"category","label":"Module 1 - ROS 2","items":[{"type":"doc","id":"module-1-ros2/introduction"},{"type":"doc","id":"module-1-ros2/architecture"},{"type":"doc","id":"module-1-ros2/code_examples"},{"type":"doc","id":"module-1-ros2/urdf"},{"type":"doc","id":"module-1-ros2/packages"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2 - Gazebo and Unity","items":[{"type":"doc","id":"module-2-gazebo/introduction"},{"type":"doc","id":"module-2-gazebo/setup"},{"type":"doc","id":"module-2-gazebo/physics"},{"type":"doc","id":"module-2-gazebo/sensors"},{"type":"doc","id":"module-2-gazebo/unity"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3 - NVIDIA Isaac","items":[{"type":"doc","id":"module-3-isaac/introduction"},{"type":"doc","id":"module-3-isaac/isaac_sim"},{"type":"doc","id":"module-3-isaac/synthetic_data"},{"type":"doc","id":"module-3-isaac/vslam"},{"type":"doc","id":"module-3-isaac/rl"},{"type":"doc","id":"module-3-isaac/sim2real"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4 - Vision-Language-Action","items":[{"type":"doc","id":"module-4-vla/introduction"},{"type":"doc","id":"module-4-vla/whisper"},{"type":"doc","id":"module-4-vla/llm_planning"},{"type":"doc","id":"module-4-vla/reasoning"}],"collapsed":true,"collapsible":true},{"type":"category","label":"Capstone Project","items":[{"type":"doc","id":"capstone/guide"}],"collapsed":true,"collapsible":true}]}}]}},"docusaurus-plugin-content-blog":{"default":{"blogSidebarTitle":"Recent posts","blogPosts":[],"blogListPaginated":[],"blogTags":{},"blogTagsListPath":"/blog/tags"}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/","source":"@site/src/pages/index.tsx"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}