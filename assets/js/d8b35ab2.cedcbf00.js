"use strict";(self.webpackChunkhumanoid_robotics_book=self.webpackChunkhumanoid_robotics_book||[]).push([[804],{3923:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/","label":"Physical AI & Humanoid Robotics","docId":"index","unlisted":false},{"type":"category","label":"Module 1 - ROS 2","items":[{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-1-ros2/introduction","label":"Module 1: The Robotic Nervous System - ROS 2","docId":"module-1-ros2/introduction","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-1-ros2/architecture","label":"ROS 2 Architecture","docId":"module-1-ros2/architecture","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-1-ros2/code_examples","label":"rclpy Code Examples","docId":"module-1-ros2/code_examples","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-1-ros2/urdf","label":"URDF for Humanoid Robots","docId":"module-1-ros2/urdf","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-1-ros2/packages","label":"Creating and Running ROS 2 Packages","docId":"module-1-ros2/packages","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2 - Gazebo and Unity","items":[{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-2-gazebo/introduction","label":"Module 2: The Digital Twin - Gazebo and Unity","docId":"module-2-gazebo/introduction","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-2-gazebo/setup","label":"Setting up Gazebo and Building Environments","docId":"module-2-gazebo/setup","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-2-gazebo/physics","label":"Physics Concepts in Robotics Simulation","docId":"module-2-gazebo/physics","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-2-gazebo/sensors","label":"Simulating Sensors in Gazebo","docId":"module-2-gazebo/sensors","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-2-gazebo/unity","label":"High-Fidelity Simulation with Unity","docId":"module-2-gazebo/unity","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3 - NVIDIA Isaac","items":[{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-3-isaac/introduction","label":"Module 3: The AI-Robot Brain - NVIDIA Isaac","docId":"module-3-isaac/introduction","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-3-isaac/isaac_sim","label":"Using NVIDIA Isaac Sim","docId":"module-3-isaac/isaac_sim","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-3-isaac/synthetic_data","label":"Generating Synthetic Data with NVIDIA Isaac Sim","docId":"module-3-isaac/synthetic_data","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-3-isaac/vslam","label":"VSLAM and Navigation with NVIDIA Isaac","docId":"module-3-isaac/vslam","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-3-isaac/rl","label":"Reinforcement Learning for Robotics with NVIDIA Isaac","docId":"module-3-isaac/rl","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-3-isaac/sim2real","label":"The Sim-to-Real Pipeline","docId":"module-3-isaac/sim2real","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4 - Vision-Language-Action","items":[{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-4-vla/introduction","label":"Module 4: Vision-Language-Action","docId":"module-4-vla/introduction","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-4-vla/whisper","label":"Voice Recognition with Whisper","docId":"module-4-vla/whisper","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-4-vla/llm_planning","label":"LLM Planning for Robotics","docId":"module-4-vla/llm_planning","unlisted":false},{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-4-vla/reasoning","label":"Multi-Step Reasoning for Robotics","docId":"module-4-vla/reasoning","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Capstone Project","items":[{"type":"link","href":"/Hackathon-Physical-AI-Humanoid-Book/docs/capstone/guide","label":"Capstone: Humanoid Robot Simulation","docId":"capstone/guide","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"capstone/guide":{"id":"capstone/guide","title":"Capstone: Humanoid Robot Simulation","description":"This capstone project will guide you through building a complete simulation of a humanoid robot that integrates all the concepts learned in the previous modules. You will create a robot that can hear a voice command, understand it, plan a path, navigate obstacles, identify an object, and interact with the environment.","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"Physical AI & Humanoid Robotics","description":"Welcome to the Physical AI & Humanoid Robotics textbook. This comprehensive guide covers the essential concepts and practical implementations for building, simulating, and deploying AI systems in physical humanoid robots using ROS 2, Gazebo, Unity, and NVIDIA Isaac.","sidebar":"tutorialSidebar"},"module-1-ros2/architecture":{"id":"module-1-ros2/architecture","title":"ROS 2 Architecture","description":"ROS 2 employs a distributed architecture where different software components communicate with each other through a middleware layer. The core concepts of ROS 2 architecture include:","sidebar":"tutorialSidebar"},"module-1-ros2/code_examples":{"id":"module-1-ros2/code_examples","title":"rclpy Code Examples","description":"This page provides code examples for using rclpy, the Python client library for ROS 2.","sidebar":"tutorialSidebar"},"module-1-ros2/introduction":{"id":"module-1-ros2/introduction","title":"Module 1: The Robotic Nervous System - ROS 2","description":"Welcome to the first module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore the foundations of robotic middleware using ROS 2 (Robot Operating System 2).","sidebar":"tutorialSidebar"},"module-1-ros2/packages":{"id":"module-1-ros2/packages","title":"Creating and Running ROS 2 Packages","description":"In ROS 2, code is organized into packages. A package is a directory that contains a package.xml file, which describes the package, and a CMakeLists.txt file, which specifies how to build the package.","sidebar":"tutorialSidebar"},"module-1-ros2/urdf":{"id":"module-1-ros2/urdf","title":"URDF for Humanoid Robots","description":"URDF (Unified Robot Description Format) is an XML file format used in ROS to describe the physical structure of a robot. A URDF file specifies the links, joints, and visual and collision properties of a robot.","sidebar":"tutorialSidebar"},"module-2-gazebo/introduction":{"id":"module-2-gazebo/introduction","title":"Module 2: The Digital Twin - Gazebo and Unity","description":"Welcome to the second module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore the world of robot simulation using Gazebo and Unity.","sidebar":"tutorialSidebar"},"module-2-gazebo/physics":{"id":"module-2-gazebo/physics","title":"Physics Concepts in Robotics Simulation","description":"In robotics simulation, understanding fundamental physics concepts is crucial for creating realistic and accurate simulations. Here, we will explore gravity, collision, and friction.","sidebar":"tutorialSidebar"},"module-2-gazebo/sensors":{"id":"module-2-gazebo/sensors","title":"Simulating Sensors in Gazebo","description":"Robots rely on sensors to perceive their environment. In simulation, we can model different types of sensors to mimic real-world sensor data. Here, we will explore simulating LiDAR, IMU, and depth cameras in Gazebo.","sidebar":"tutorialSidebar"},"module-2-gazebo/setup":{"id":"module-2-gazebo/setup","title":"Setting up Gazebo and Building Environments","description":"Gazebo is a popular open-source robot simulator that allows you to create realistic 3D environments and simulate the behavior of robots in those environments.","sidebar":"tutorialSidebar"},"module-2-gazebo/unity":{"id":"module-2-gazebo/unity","title":"High-Fidelity Simulation with Unity","description":"Unity is a powerful game engine that can also be used for high-fidelity robotics simulation. It provides a rich set of tools and features for creating realistic and interactive environments.","sidebar":"tutorialSidebar"},"module-3-isaac/introduction":{"id":"module-3-isaac/introduction","title":"Module 3: The AI-Robot Brain - NVIDIA Isaac","description":"Welcome to the third module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore the use of NVIDIA Isaac for AI perception and navigation in robotics.","sidebar":"tutorialSidebar"},"module-3-isaac/isaac_sim":{"id":"module-3-isaac/isaac_sim","title":"Using NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a powerful, photorealistic robot simulator that is part of the NVIDIA Isaac robotics platform. It provides a virtual environment for developing, testing, and training AI-powered robots.","sidebar":"tutorialSidebar"},"module-3-isaac/rl":{"id":"module-3-isaac/rl","title":"Reinforcement Learning for Robotics with NVIDIA Isaac","description":"Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions in an environment to maximize a reward signal. RL is well-suited for robotics tasks, as it allows robots to learn complex behaviors through trial and error.","sidebar":"tutorialSidebar"},"module-3-isaac/sim2real":{"id":"module-3-isaac/sim2real","title":"The Sim-to-Real Pipeline","description":"The sim-to-real pipeline refers to the process of transferring AI models trained in simulation to real-world robots. This is a crucial step in robotics development, as it allows you to leverage the benefits of simulation while still deploying your models on real robots.","sidebar":"tutorialSidebar"},"module-3-isaac/synthetic_data":{"id":"module-3-isaac/synthetic_data","title":"Generating Synthetic Data with NVIDIA Isaac Sim","description":"Synthetic data is artificially created data that can be used to train AI models. Generating synthetic data is a powerful technique for robotics, as it allows you to create large datasets of labeled data without the need for real-world data collection.","sidebar":"tutorialSidebar"},"module-3-isaac/vslam":{"id":"module-3-isaac/vslam","title":"VSLAM and Navigation with NVIDIA Isaac","description":"VSLAM (Visual Simultaneous Localization and Mapping) is a technique used by robots to build a map of their environment while simultaneously estimating their pose within that map. Navigation builds upon the generated map, allowing the robot to plan and execute paths to reach desired goals. NVIDIA Isaac provides tools for implementing VSLAM and navigation algorithms.","sidebar":"tutorialSidebar"},"module-4-vla/introduction":{"id":"module-4-vla/introduction","title":"Module 4: Vision-Language-Action","description":"Welcome to the fourth module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore how to combine vision, language, and action to create robots that can understand and interact with the world in a more natural way.","sidebar":"tutorialSidebar"},"module-4-vla/llm_planning":{"id":"module-4-vla/llm_planning","title":"LLM Planning for Robotics","description":"Large Language Models (LLMs) can be used to generate robot plans from natural language commands. This allows robots to perform complex tasks without the need for explicit programming.","sidebar":"tutorialSidebar"},"module-4-vla/reasoning":{"id":"module-4-vla/reasoning","title":"Multi-Step Reasoning for Robotics","description":"Multi-step reasoning is a critical capability for robots that need to perform complex tasks. It involves breaking down a high-level goal into a sequence of smaller, more manageable sub-tasks, and then executing those sub-tasks in a logical order.","sidebar":"tutorialSidebar"},"module-4-vla/whisper":{"id":"module-4-vla/whisper","title":"Voice Recognition with Whisper","description":"Whisper is a neural network developed by OpenAI that transcribes spoken language into text. It is a powerful tool for enabling robots to understand and respond to human voice commands.","sidebar":"tutorialSidebar"}}}}')}}]);