"use strict";(self.webpackChunkhumanoid_robotics_book=self.webpackChunkhumanoid_robotics_book||[]).push([[187],{1318:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>u,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/introduction","title":"Module 4: Vision-Language-Action","description":"Welcome to the fourth module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore how to combine vision, language, and action to create robots that can understand and interact with the world in a more natural way.","source":"@site/docs/module-4-vla/introduction.mdx","sourceDirName":"module-4-vla","slug":"/module-4-vla/introduction","permalink":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-4-vla/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/lotusf980/Hackathon-Physical-AI-Humanoid-Book/edit/main/docs/module-4-vla/introduction.mdx","tags":[],"version":"current","frontMatter":{"title":"Module 4: Vision-Language-Action"},"sidebar":"tutorialSidebar","previous":{"title":"The Sim-to-Real Pipeline","permalink":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-3-isaac/sim2real"},"next":{"title":"Voice Recognition with Whisper","permalink":"/Hackathon-Physical-AI-Humanoid-Book/docs/module-4-vla/whisper"}}');var i=o(4848),a=o(8453);const l={title:"Module 4: Vision-Language-Action"},s="Module 4: Vision-Language-Action",r={},c=[];function d(e){const n={h1:"h1",header:"header",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"module-4-vision-language-action",children:"Module 4: Vision-Language-Action"})}),"\n",(0,i.jsx)(n.p,{children:"Welcome to the fourth module of the Physical AI & Humanoid Robotics textbook! In this module, we will explore how to combine vision, language, and action to create robots that can understand and interact with the world in a more natural way."}),"\n",(0,i.jsx)(n.p,{children:"Vision-Language-Action (VLA) is an emerging field that focuses on building robots that can:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand natural language commands."}),"\n",(0,i.jsx)(n.li,{children:"Perceive their environment through vision."}),"\n",(0,i.jsx)(n.li,{children:"Plan and execute actions to achieve desired goals."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In this module, you will learn about:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Whisper voice recognition: Transcribe spoken language into text."}),"\n",(0,i.jsx)(n.li,{children:'LLM planning ("Clean the room" -> actions): Use large language models to generate robot plans from natural language commands.'}),"\n",(0,i.jsx)(n.li,{children:"Multi-step reasoning for robotics: Enable robots to reason about complex tasks that require multiple steps."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"By the end of this module, you will be able to build a simulated humanoid robot that: Hears a voice command, Understands it, Plans a path, Navigates obstacles, Identifies an object, Interacts with the environment."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>l,x:()=>s});var t=o(6540);const i={},a=t.createContext(i);function l(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);